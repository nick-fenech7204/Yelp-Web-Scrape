{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from bs4 import SoupStrainer\n",
    "from urllib.parse import unquote\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_link(): \n",
    "    print('''Hello! This webscraper will pull all relevant business information for you, \n",
    "please enter the information below (Be aware this may take up to 30 minutes to complete, \n",
    "also please no numbers or symbols).''')\n",
    "    print('===========================================')\n",
    "    global industry\n",
    "    global city\n",
    "    global state\n",
    "    while True: \n",
    "        try:\n",
    "            industry = input('Please type in an Industry: ').capitalize().replace(' ', '')\n",
    "            city = input('Please type in a City: ').capitalize().replace(' ', '')\n",
    "            state = input('Please type in the acronym of the city\\'s state (New York = NY): ').strip().upper()\n",
    "            \n",
    "            if industry.isalpha() and city.isalpha() and state.isalpha() and len(state) == 2:\n",
    "                test_url = f\"https://www.yelp.com/search?find_desc={industry}&find_loc={city}%2C+{state}&start=\"\n",
    "                break\n",
    "            else:\n",
    "                print('I\\'m sorry, one of your responses was invalid. Please try again.')\n",
    "        except ValueError:\n",
    "            print('I\\'m sorry, one of your responses was invalid. Please try again.')\n",
    "        \n",
    "    return test_url\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indv_business_links(url_needed): \n",
    "    \n",
    "    print(\"Step 1: Obtaining all the links from each individual page.\")\n",
    "    global page_numbers\n",
    "    page_numbers = range(0,10,10)\n",
    "    link_lst = []\n",
    "    #initiate a blank list for the links\n",
    "\n",
    "    for page_number in page_numbers:\n",
    "\n",
    "        url = f'{url_needed}{page_number}'\n",
    "        result = requests.get(url).text \n",
    "        doc = BeautifulSoup(result, 'html.parser')\n",
    "        #parse html down to the link string\n",
    "        body = doc.find('body')\n",
    "        main = body.find('ul')\n",
    "        title_link = main.find_all('h3')\n",
    "\n",
    "        for i in title_link:\n",
    "            a_tag = i.find('a')\n",
    "            #parsed down to the correct html area\n",
    "            if a_tag:\n",
    "                href_value = a_tag.get('href')\n",
    "                link = 'https://yelp.com'+ href_value\n",
    "                link_lst.append(link)\n",
    "                # pull the link from href and contcatinating https://yelp.com + link\n",
    "    \n",
    "    return link_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getting_org_data(list_of_links): \n",
    "    \n",
    "    print('Step 2: Working on Getting all the information from each page.')\n",
    "    contact_list = []\n",
    "\n",
    "    for url2 in list_of_links:\n",
    "\n",
    "        page_result = requests.get(url2).text\n",
    "        info_pages= BeautifulSoup(page_result, 'html.parser')\n",
    "        #pulling liinks from list and iterating through for each link\n",
    "        body2 = info_pages.find('body')\n",
    "        aside = body2.find('aside')\n",
    "        #parsing down the heirarchy of html docs to the block containing the information needed\n",
    "        try:\n",
    "            if aside:\n",
    "                phone_number = aside.find_all('p', class_ = 'css-1p9ibgf')\n",
    "                address = aside.find_all('p', class_ = 'css-qyp8bo')\n",
    "                business_website = aside.find('a')\n",
    "                #parsing down to the correct html script that contains the string value \n",
    "                spcbody = body2.find_all('a', class_='css-19v1rkv') \n",
    "\n",
    "                spclst = []\n",
    "                for x in spcbody:\n",
    "                    if 'find_desc' in x.get('href'):\n",
    "                        spclst.append(x.text)\n",
    "\n",
    "                if business_website:\n",
    "                    main = body2.find('main')\n",
    "                    business_name = body2.find('h1', class_='css-1se8maq').text\n",
    "                    business_website2 = business_website.get('href')\n",
    "                    bizzy_website = business_website2.removeprefix('/biz_redir?url=')\n",
    "                    decoded_url = unquote(bizzy_website).split('&')\n",
    "                    # conducting string manipulation for the correct link to the business \n",
    "                    contact_list.append((business_name,decoded_url[0],phone_number[1].text,\n",
    "                    address[0].text,', '.join(spclst)))\n",
    "                    #appedning the sliced string of each part if html text infomation, \n",
    "\n",
    "        except (AttributeError, IndexError, ValueError): \n",
    "            pass \n",
    "    \n",
    "    return contact_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data):\n",
    "\n",
    "    print(f'Step 3: Done! You can now see a CSV file and Excel file for {industry} in {city}, {state} within your Downloads folder.') \n",
    "    headers2 = ['Business Name', 'Website', 'Phone', 'Address', 'Specialty']\n",
    "    global df2\n",
    "    df = pd.DataFrame(data, columns= headers2)\n",
    "    df = df.set_index(\"Business Name\")\n",
    "    df = df.replace(to_replace='Get Directions', value= 'Phone Number Unavailable')\n",
    "    df = df.replace(r'^/map/.*', 'Website Unavailable', regex=True)\n",
    "    df2 = df\n",
    "\n",
    "    downloads_folder = os.path.expanduser(\"~\") + os.sep + \"Downloads\"\n",
    "    filepath_csv = os.path.join(downloads_folder, f\"{industry}_in_{city},{state}--yelp_scrap.csv\")\n",
    "    filepath_excel = os.path.join(downloads_folder, f\"{industry}_in_{city},{state}--yelp_scrap.xlsx\")\n",
    "\n",
    "    df2.to_csv(filepath_csv)\n",
    "    df2.to_excel(filepath_excel)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_dataframe(getting_org_data(get_indv_business_links(construct_link())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenviorment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
